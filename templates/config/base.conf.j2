{%- import '_macros.j2' as macros with context -%}

{% set config = elao_influxdb_config -%}

{% set config_meta = [] -%}
{% set config_data = [] -%}
{% set config_database = [] -%}
{% set config_hinted_handoff = [] -%}
{% set config_cluster = [] -%}
{% set config_retention = [] -%}
{% set config_shard_precreation = [] -%}
{% set config_monitor = [] -%}
{% set config_admin = [] -%}
{% set config_http = [] -%}
{% set config_graphite = [] -%}
{% set config_collectd = [] -%}
{% set config_opentsdb = [] -%}
{% set config_udp = [] -%}
{% set config_continuous_queries = [] -%}

{%- for configs in config -%}
    {%- for config_name, config_parameters in configs.iteritems() -%}
        {%- if config_name == 'meta' -%}
            {%- if config_meta.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'data' -%}
            {%- if config_data.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'database' -%}
            {%- if config_database.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'database' -%}
            {%- if config_database.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'hinted_handoff' -%}
            {%- if config_hinted_handoff.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'retention' -%}
            {%- if config_retention.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'shard_precreation' -%}
            {%- if config_shard_precreation.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'monitor' -%}
            {%- if config_monitor.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'admin' -%}
            {%- if config_admin.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'http' -%}
            {%- if config_http.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'auth.graphite' -%}
            {%- if config_graphite.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'collectd' -%}
            {%- if config_collectd.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'udp' -%}
            {%- if config_udp.extend(config_parameters) -%}{%- endif -%}
        {%- elif config_name == 'continuous_queries' -%}
            {%- if config_continuous_queries.extend(config_parameters) -%}{%- endif -%}
        {%- endif -%}
    {%- endfor -%}
{%- endfor -%}

### Welcome to the InfluxDB configuration file.

# Once every 24 hours InfluxDB will report anonymous data to m.influxdb.com
# The data includes raft id (random 8 bytes), os, arch, version, and metadata.
# We don't track ip addresses of servers reporting. This is only used
# to track the number of instances running and the versions, which
# is very helpful for us.
# Change this option to true to disable reporting.
{{ macros.config_row(config, 'reporting-disabled', false) }}

# we'll try to get the hostname automatically, but if it the os returns something
# that isn't resolvable by other servers in the cluster, use this option to
# manually set the hostname
{{ macros.config_row(config, 'hostname', '# hostname = "localhost"', 0, true) }}

###
### [meta]
###
### Controls the parameters for the Raft consensus group that stores metadata
### about the InfluxDB cluster.
###

[meta]
  # Controls if this node should run the metaservice and participate in the Raft group
  {{ macros.config_row(config_meta, 'enabled', true) }}

  # Where the metadata/raft database is stored
  {{ macros.config_row(config_meta, 'dir', '/var/lib/influxdb/meta') }}

  {{ macros.config_row(config_meta, 'bind-address', ':8088') }}
  {{ macros.config_row(config_meta, 'retention-autocreate', true) }}
  {{ macros.config_row(config_meta, 'election-timeout', '1s') }}
  {{ macros.config_row(config_meta, 'heartbeat-timeout', '1s') }}
  {{ macros.config_row(config_meta, 'leader-lease-timeout', '500ms') }}
  {{ macros.config_row(config_meta, 'commit-timeout', '50ms') }}
  {{ macros.config_row(config_meta, 'cluster-tracing', false) }}

###
### [data]
###
### Controls where the actual shard data for InfluxDB lives and how it is
### flushed from the WAL. "dir" may need to be changed to a suitable place
### for your system, but the WAL settings are an advanced configuration. The
### defaults should work for most systems.
###

[data]
  # Controls if this node holds time series data shards in the cluster
  {{ macros.config_row(config_data, 'enabled', true) }}

  {{ macros.config_row(config_data, 'dir', '/var/lib/influxdb/data') }}

  # The following WAL settings are for the b1 storage engine used in 0.9.2. They won't
  # apply to any new shards created after upgrading to a version > 0.9.3.
  {{ macros.config_row(config_data, 'max-wal-size', 'max-wal-size = 104857600 # Maximum size the WAL can reach before a flush. Defaults to 100MB.', 0, true) }}
  {{ macros.config_row(config_data, 'wal-flush-interval', 'wal-flush-interval = "10m" # Maximum time data can sit in WAL before a flush.', 0, true) }}
  {{ macros.config_row(config_data, 'wal-partition-flush-delay', 'wal-partition-flush-delay = "2s" # The delay time between each WAL partition being flushed.', 0, true) }}

  # These are the WAL settings for the storage engine >= 0.9.3
  {{ macros.config_row(config_data, 'wal-dir', '/var/lib/influxdb/wal') }}
  {{ macros.config_row(config_data, 'wal-logging-enabled', true) }}
  {{ macros.config_row(config_data, 'data-logging-enabled', true) }}

  # When a series in the WAL in-memory cache reaches this size in bytes it is marked as ready to
  # flush to the index
  {{ macros.config_row(config_data, 'wal-ready-series-size', '# wal-ready-series-size = 25600', 0, true) }}

  # Flush and compact a partition once this ratio of series are over the ready size
  {{ macros.config_row(config_data, 'wal-compaction-threshold', '# wal-compaction-threshold = 0.6', 0, true) }}

  # Force a flush and compaction if any series in a partition gets above this size in bytes
  {{ macros.config_row(config_data, 'wal-max-series-size', '# wal-max-series-size = 2097152', 0, true) }}

  # Force a flush of all series and full compaction if there have been no writes in this
  # amount of time. This is useful for ensuring that shards that are cold for writes don't
  # keep a bunch of data cached in memory and in the WAL.
  {{ macros.config_row(config_data, 'wal-flush-cold-interval', '# wal-flush-cold-interval = "10m"', 0, true) }}

  # Force a partition to flush its largest series if it reaches this approximate size in
  # bytes. Remember there are 5 partitions so you'll need at least 5x this amount of memory.
  # The more memory you have, the bigger this can be.
  {{ macros.config_row(config_data, 'wal-partition-size-threshold', '# wal-partition-size-threshold = 20971520', 0, true) }}

  # Whether queries should be logged before execution. Very useful for troubleshooting, but will
  # log any sensitive data contained within a query.
  {{ macros.config_row(config_data, 'query-log-enabled', '# query-log-enabled = true', 0, true) }}

  # Settings for the TSM engine

  # CacheMaxMemorySize is the maximum size a shard's cache can
  # reach before it starts rejecting writes.
  {{ macros.config_row(config_data, 'cache-max-memory-size', '# cache-max-memory-size = 524288000', 0, true) }}

  # CacheSnapshotMemorySize is the size at which the engine will
  # snapshot the cache and write it to a TSM file, freeing up memory
  {{ macros.config_row(config_data, 'cache-snapshot-memory-size', '# cache-snapshot-memory-size = 26214400', 0, true) }}

  # CacheSnapshotWriteColdDuration is the length of time at
  # which the engine will snapshot the cache and write it to
  # a new TSM file if the shard hasn't received writes or deletes
  {{ macros.config_row(config_data, 'cache-snapshot-write-cold-duration', '# cache-snapshot-write-cold-duration = "1h"', 0, true) }}

  # MinCompactionFileCount is the minimum number of TSM files
  # that need to exist before a compaction cycle will run
  {{ macros.config_row(config_data, 'compact-min-file-count', '# compact-min-file-count = 3', 0, true) }}

  # CompactFullWriteColdDuration is the duration at which the engine
  # will compact all TSM files in a shard if it hasn't received a
  # write or delete
  {{ macros.config_row(config_data, 'compact-full-write-cold-duration', '# compact-full-write-cold-duration = "24h"', 0, true) }}

  # MaxPointsPerBlock is the maximum number of points in an encoded
  # block in a TSM file. Larger numbers may yield better compression
  # but could incur a performance peanalty when querying
  {{ macros.config_row(config_data, 'max-points-per-block', '# max-points-per-block = 1000', 0, true) }}

###
### [hinted-handoff]
###
### Controls the hinted handoff feature, which allows nodes to temporarily
### store queued data when one node of a cluster is down for a short period
### of time.
###

[hinted-handoff]
  {{ macros.config_row(config_hinted_handoff, 'enabled', true) }}
  {{ macros.config_row(config_hinted_handoff, 'dir', '/var/lib/influxdb/hh') }}
  {{ macros.config_row(config_hinted_handoff, 'max-size', 1073741824) }}
  {{ macros.config_row(config_hinted_handoff, 'max-age', '168h') }}
  {{ macros.config_row(config_hinted_handoff, 'retry-rate-limit', 0) }}

  # Hinted handoff will start retrying writes to down nodes at a rate of once per second.
  # If any error occurs, it will backoff in an exponential manner, until the interval
  # reaches retry-max-interval. Once writes to all nodes are successfully completed the
  # interval will reset to retry-interval.
  {{ macros.config_row(config_hinted_handoff, 'retry-interval', '1s') }}
  {{ macros.config_row(config_hinted_handoff, 'retry-max-interval', '1m') }}

  # Interval between running checks for data that should be purged. Data is purged from
  # hinted-handoff queues for two reasons. 1) The data is older than the max age, or
  # 2) the target node has been dropped from the cluster. Data is never dropped until
  # it has reached max-age however, for a dropped node or not.
  {{ macros.config_row(config_hinted_handoff, 'purge-interval', '1h') }}

###
### [cluster]
###
### Controls non-Raft cluster behavior, which generally includes how data is
### shared across shards.
###

[cluster]
  {{ macros.config_row(config_cluster, 'shard-writer-timeout', 'shard-writer-timeout = "5s" # The time within which a remote shard must respond to a write request.', 0, true) }}
  {{ macros.config_row(config_cluster, 'write-timeout', 'write-timeout = "10s" # The time within which a write request must complete on the cluster.', 0, true) }}

###
### [retention]
###
### Controls the enforcement of retention policies for evicting old data.
###

[retention]
  {{ macros.config_row(config_retention, 'enabled', true) }}
  {{ macros.config_row(config_retention, 'check-interval', '30m') }}

###
### [shard-precreation]
###
### Controls the precreation of shards, so they are available before data arrives.
### Only shards that, after creation, will have both a start- and end-time in the
### future, will ever be created. Shards are never precreated that would be wholly
### or partially in the past.

[shard-precreation]
  {{ macros.config_row(config_shard_precreation, 'enabled', true) }}
  {{ macros.config_row(config_shard_precreation, 'check-interval', '10m') }}
  {{ macros.config_row(config_shard_precreation, 'advance-period', '30m') }}

###
### Controls the system self-monitoring, statistics and diagnostics.
###
### The internal database for monitoring data is created automatically if
### if it does not already exist. The target retention within this database
### is called 'monitor' and is also created with a retention period of 7 days
### and a replication factor of 1, if it does not exist. In all cases the
### this retention policy is configured as the default for the database.

[monitor]
  {{ macros.config_row(config_monitor, 'store-enabled', 'store-enabled = true # Whether to record statistics internally.', 0, true) }}
  {{ macros.config_row(config_monitor, 'store-database', 'store-database = "_internal" # The destination database for recorded statistics', 0, true) }}
  {{ macros.config_row(config_monitor, 'store-interval', 'store-interval = "10s" # The interval at which to record statistics', 0, true) }}

###
### [admin]
###
### Controls the availability of the built-in, web-based admin interface. If HTTPS is
### enabled for the admin interface, HTTPS must also be enabled on the [http] service.
###

[admin]
  {{ macros.config_row(config_admin, 'enabled', true) }}
  {{ macros.config_row(config_admin, 'bind-address', ':8083') }}
  {{ macros.config_row(config_admin, 'https-enabled', false) }}
  {{ macros.config_row(config_admin, 'https-certificate', '/etc/ssl/influxdb.pem') }}

###
### [http]
###
### Controls how the HTTP endpoints are configured. These are the primary
### mechanism for getting data into and out of InfluxDB.
###

[http]
  {{ macros.config_row(config_http, 'enabled', true) }}
  {{ macros.config_row(config_http, 'bind-address', ':8086') }}
  {{ macros.config_row(config_http, 'auth-enabled', false) }}
  {{ macros.config_row(config_http, 'log-enabled', true) }}
  {{ macros.config_row(config_http, 'write-tracing', false) }}
  {{ macros.config_row(config_http, 'pprof-enabled', false) }}
  {{ macros.config_row(config_http, 'https-enabled', false) }}
  {{ macros.config_row(config_http, 'https-certificate', '/etc/ssl/influxdb.pem') }}

###
### [[graphite]]
###
### Controls one or many listeners for Graphite data.
###

[[graphite]]
  {{ macros.config_row(config_graphite, 'enabled', false) }}
  {{ macros.config_row(config_graphite, 'database', '# database = "graphite"', 0, true) }}
  {{ macros.config_row(config_graphite, 'bind-address', '# bind-address = ":2003"', 0, true) }}
  {{ macros.config_row(config_graphite, 'protocol', '# protocol = "tcp"', 0, true) }}
  {{ macros.config_row(config_graphite, 'consistency-level', '# consistency-level = "one"', 0, true) }}
  {{ macros.config_row(config_graphite, 'name-separator', '# name-separator = "."', 0, true) }}

  # These next lines control how batching works. You should have this enabled
  # otherwise you could get dropped metrics or poor performance. Batching
  # will buffer points in memory if you have many coming in.

  {{ macros.config_row(config_graphite, 'batch-size', '# batch-size = 1000 # will flush if this many points get buffered', 0, true) }}
  {{ macros.config_row(config_graphite, 'batch-pending', '# batch-pending = 5 # number of batches that may be pending in memory', 0, true) }}
  {{ macros.config_row(config_graphite, 'batch-timeout', '# batch-timeout = "1s" # will flush at least this often even if we haven\'t hit buffer limit', 0, true) }}
  {{ macros.config_row(config_graphite, 'udp-read-buffer', '# udp-read-buffer = 0 # UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.', 0, true) }}

  ## "name-schema" configures tag names for parsing the metric name from graphite protocol;
  ## separated by `name-separator`.
  ## The "measurement" tag is special and the corresponding field will become
  ## the name of the metric.
  ## e.g. "type.host.measurement.device" will parse "server.localhost.cpu.cpu0" as
  ## {
  ##     measurement: "cpu",
  ##     tags: {
  ##         "type": "server",
  ##         "host": "localhost,
  ##         "device": "cpu0"
  ##     }
  ## }
  {{ macros.config_row(config_graphite, 'name-schema', '# name-schema = "type.host.measurement.device"', 0, true) }}

  ## If set to true, when the input metric name has more fields than `name-schema` specified,
  ## the extra fields will be ignored.
  ## Otherwise an error will be logged and the metric rejected.
  {{ macros.config_row(config_graphite, 'ignore-unnamed', '# ignore-unnamed = true', 0, true) }}

###
### [collectd]
###
### Controls the listener for collectd data.
###

[collectd]
  {{ macros.config_row(config_collectd, 'enabled', 'enabled = false', 0, true) }}
  {{ macros.config_row(config_collectd, 'bind-address', '# bind-address = ""', 0, true) }}
  {{ macros.config_row(config_collectd, 'database', '# database = ""', 0, true) }}
  {{ macros.config_row(config_collectd, 'typesdb', '# typesdb = ""', 0, true) }}

  # These next lines control how batching works. You should have this enabled
  # otherwise you could get dropped metrics or poor performance. Batching
  # will buffer points in memory if you have many coming in.

  {{ macros.config_row(config_collectd, 'batch-size', '# batch-size = 1000 # will flush if this many points get buffered', 0, true) }}
  {{ macros.config_row(config_collectd, 'batch-pending', '# batch-pending = 5 # number of batches that may be pending in memory', 0, true) }}
  {{ macros.config_row(config_collectd, 'batch-timeout', '# batch-timeout = "1s" # will flush at least this often even if we haven\'t hit buffer limit', 0, true) }}
  {{ macros.config_row(config_collectd, 'read-buffer', '# read-buffer = 0 # UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.', 0, true) }}

###
### [opentsdb]
###
### Controls the listener for OpenTSDB data.
###

[opentsdb]
  {{ macros.config_row(config_opentsdb, 'enabled', false) }}
  {{ macros.config_row(config_opentsdb, 'bind-address', '# bind-address = ":4242"', 0, true) }}
  {{ macros.config_row(config_opentsdb, 'database', '# database = "opentsdb"', 0, true) }}
  {{ macros.config_row(config_opentsdb, 'retention-policy', '# retention-policy = ""', 0, true) }}
  {{ macros.config_row(config_opentsdb, 'consistency-level', '# consistency-level = "one"', 0, true) }}
  {{ macros.config_row(config_opentsdb, 'tls-enabled', '# tls-enabled = false', 0, true) }}
  {{ macros.config_row(config_opentsdb, 'certificate', '# certificate= ""', 0, true) }}
  {{ macros.config_row(config_opentsdb, 'log-point-errors', '# log-point-errors = true # Log an error for every malformed point.', 0, true) }}

  # These next lines control how batching works. You should have this enabled
  # otherwise you could get dropped metrics or poor performance. Only points
  # metrics received over the telnet protocol undergo batching.

  {{ macros.config_row(config_opentsdb, 'batch-size', '# batch-size = 1000 # will flush if this many points get buffered', 0, true) }}
  {{ macros.config_row(config_opentsdb, 'batch-pending', '# batch-pending = 5 # number of batches that may be pending in memory', 0, true) }}
  {{ macros.config_row(config_opentsdb, 'batch-timeout', '# batch-timeout = "1s" # will flush at least this often even if we haven\'t hit buffer limit', 0, true) }}

###
### [[udp]]
###
### Controls the listeners for InfluxDB line protocol data via UDP.
###

[[udp]]
  {{ macros.config_row(config_udp, 'enabled', false) }}
  {{ macros.config_row(config_udp, 'bind-address', '# bind-address = ""', 0, true) }}
  {{ macros.config_row(config_udp, 'database', '# database = "udp"', 0, true) }}
  {{ macros.config_row(config_udp, 'retention-policy', '# retention-policy = ""', 0, true) }}

  # These next lines control how batching works. You should have this enabled
  # otherwise you could get dropped metrics or poor performance. Batching
  # will buffer points in memory if you have many coming in.

  {{ macros.config_row(config_udp, 'batch-size', '# batch-size = 1000 # will flush if this many points get buffered', 0, true) }}
  {{ macros.config_row(config_udp, 'batch-pending', '# batch-pending = 5 # number of batches that may be pending in memory', 0, true) }}
  {{ macros.config_row(config_udp, 'batch-timeout', '# batch-timeout = "1s" # will flush at least this often even if we haven\'t hit buffer limit', 0, true) }}
  {{ macros.config_row(config_udp, 'read-buffer', '# read-buffer = 0 # UDP Read buffer size, 0 means OS default. UDP listener will fail if set above OS max.', 0, true) }}

  # set the expected UDP payload size; lower values tend to yield better performance, default is max UDP size 65536
  {{ macros.config_row(config_udp, 'udp-payload-size', '# udp-payload-size = 65536', 0, true) }}

###
### [continuous_queries]
###
### Controls how continuous queries are run within InfluxDB.
###

[continuous_queries]
  {{ macros.config_row(config_continuous_queries, 'log-enabled', true) }}
  {{ macros.config_row(config_continuous_queries, 'enabled', true) }}
  {{ macros.config_row(config_continuous_queries, 'run-interval', '# run-interval = "1s" # interval for how often continuous queries will be checked if they need to run', 0, true) }}
